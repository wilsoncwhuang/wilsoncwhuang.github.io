I"#3<h2 id="1-introduction">1. Introduction</h2>

<h3 id="11-overview">1.1 Overview</h3>
<p>Revenue forecasting has always been crucial for businesses to prepare for upcoming demands and long-term
strategic planning. It provides insights in demand planning, storage space allocation, human resource planning, logistics
arrangements, and more. Therefore, business owners often deploy certain data analytics models to simulate outcomes that
can capture future trends for their operations. Models used to conduct such a simulation may vary based on its required
precision or capability for dynamically adjusting to irregularities, but all models are aiming for similar results. Some
common predicting models that utilize machine learning techniques for sales forecasting include linear regression, random
forest regressions, extreme gradient boosting (XGBoost), long short-term memory (LSTM) model, ARIMA time series
forecasting, and more [1]. In this project, we will be using multivariate linear regression techniques to forecast sales trends
for targeted retail stores.</p>

<h3 id="12-subject-for-analysis---walmart-supercenters">1.2 Subject for Analysis - Walmart Supercenters</h3>
<p>For this project, we will be using regression models to forecast future sales of Walmart Supercenters within the
United States. Based on their business model [2], Walmart Supercenters provides a one-stop shopping experience by
combining grocery stores with fresh produce, bakery, deli and dairy products. They also include electronics, apparel, toys,
and home furnishing items to customers with various needs. Furthermore, most supercenters are open 24 hours with
additional services such as banks, hair, pharmacy, and more. The main source of customers for Walmart is from physical
retail stores, such as Walmart Superstores, and e-commerce sales. Based on figure 1, we can see that even though the total
number of Walmart stores has been slightly decaying, the number of supercenters is still increasing. This indicates the
significance of Walmart Supercenters in the United States Region. Therefore, an effective model to forecast future sales
for Walmart Supercenters would provide a critical advantage for senior level executives and managers to plan ahead.</p>

<p><img src="/assets/images/Walmart Sales Prediction/1-2-1.png" alt="1-2-1" title="1-2-1" /></p>

<h2 id="2-problem-statement">2. Problem Statement</h2>
<p>In this project, our goal is to predict the weekly sales of Walmart stores based on historical data with several
independent variables. Here we selected 10 independent variables that we assume may influence the weekly sales – date,
temperature, fuel price, Consumer Price Index, unemployment rate, etc. We further elaborate on these independent and
dependent variables as below. One of the biggest challenges for building an effective regression model depends on the
quality of selected factors and understanding its correlations between each predictors. As we are curious about what
factors may affect the sales revenue of a retail store in the US, we plan to conduct a series of regression analysis with
residual analysis and other data validation techniques to find out the most relevant variables. By finalizing these findings,
we can better predict the weekly sales with these selected predictors.</p>

<h2 id="3-data-description">3. Data Description</h2>
<p>We used the open-source dataset <strong>“Walmart Dataset”</strong> on Kaggle, the largest data science platform in the world.
We got the dataset as a CSV file directly from the URL address:
<a href="https://www.kaggle.com/datasets/yasserh/walmart-dataset">https://www.kaggle.com/datasets/yasserh/walmart-dataset</a>. Our dataset covers the historical sales records and other
variables of 45 Walmart stores for the period from 2010/2/5 to 2012/11/1. The sample size of the Walmart Dataset is
6,435. As we split the column <strong><em>Date</em></strong> in the Walmart Dataset into 4 different columns (<strong><em>Year</em></strong>, <strong><em>Month</em></strong>, <strong><em>Day</em></strong>, and <strong><em>Weekday</em></strong>),
the number of independent variables will be 10 and the number of dependent variables is 1 in the dataset. In addition, we
split the sample dataset into a training set and a test set with an 8:2 ratio (train = 5,148, test = 1,287) to ensure that our
prediction is unbiased. Since our current model to obtain the response variable, weekly sales for a specific Walmart store
(<strong><em>Weekly_Sales</em></strong>), is a time-dependent study with a 1-week period between each data point, we can assume the data points
we use are time-dependent. The following table summarizes the description and categories of our 11 variables:</p>

<p><img src="/assets/images/Walmart Sales Prediction/3-1.png" alt="3-1" title="3-1" /></p>

<p>For the qualitative variables, <strong><em>Holiday_Flag</em></strong> can be viewed as a dummy variable that indicates whether a specific
data point includes holidays within its period. This variable is related to our assumption that holidays may create a
temporary spike in demand, which leads to a higher weekly sales amount for the store. The other qualitative variable is
Store, which simply indicates the Walmart store number that is covered in our study. Since the study only includes a
certain amount of stores to analyze, the data directly assign each store with a numerical value between 1 to 45 (total
Walmart store within the study: 45).</p>

<h2 id="4-regression-analysis">4. Regression Analysis</h2>
<p>After basic data preprocessing, we performed the following processes for the regression analysis. First of all, we
performed some exploratory analysis on our dataset to see if there are any noteworthy characteristics or trends of the
variables. We then built a full multiple linear regression model based on all the predictors we originally have. After
analysis and diagnosis, we took different actions on the full model and built several reduced regression models to improve
the model performance, and we conducted residual analysis and checked if the basic regression assumptions hold in each
of our reduced models accordingly. Finally, we used our test data to conduct predictions on our models.</p>

<p>In the following section, R would be the main tool for our regression analysis.</p>

<h3 id="41-exploratory-analysis">4.1 Exploratory Analysis</h3>
<p>First of all, Figure 2 shows the frequency of the response variable, <strong><em>Weekly_Sales</em></strong>. The weekly sales are around 1
million dollars in most Walmart stores. It implies that the response variable would be in a great range of scale.</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-1-1.png" alt="4-1-1" title="4-1-1" /></p>

<p>We can see that Store 4, 14, 20 have higher weekly sales with larger variability in Figure 3. On the other hand, the
weekly sales are lower in Store 5, 33, 44. This might be caused by the different areas these 45 stores are located.</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-1-2.png" alt="4-1-2" title="4-1-2" /></p>

<p>In Figure 4 below, the average of weekly sales among stores is similar in each month. We notice that there are
more data points outside the 75th percentile in December than in other months. It indicates that people would purchase
much more in December, the Christmas holiday season.</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-1-3.png" alt="4-1-3" title="4-1-3" /></p>

<p>Figure 5 displays that July, September, October, November, and December should be holiday seasons with counts
of holiday flags (<strong><em>Holiday_Flag</em></strong> = 1, the white area in the histogram).</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-1-4.png" alt="4-1-4" title="4-1-4" /></p>

<h3 id="42-regression-analysis---full-model">4.2 Regression Analysis - Full Model</h3>
<p>After preprocessing and exploring the dataset, we built a full linear regression model with all predictors we
currently have. The basic information of this model is summarized in the Appendix section - Summary of Full model. We
set our significance level at 0.05, and noticed that 16 of the predictors are insignificant in the full model. In addition, the
R-squared value is 94.29% and the adjusted R-squared value is 94.18%, which is quite high and may result from
multicollinearity. Based on these facts of the full models, we want to further improve our model to enhance its prediction
performance. We accordingly performed stepwise regression for feature selection, conducted the residual analysis to test if
basic assumptions hold in the model, applied data transformation, identified potential outliers, and checked for
multicollinearity. These processes and results are elaborated on in the following paragraphs.</p>

<h3 id="43-forward-and-backward-stepwise-regression">4.3 Forward and Backward Stepwise Regression</h3>
<p>In order to avoid potential overfitting issues, we conducted forward and backward stepwise regression to perform
feature selection. This is because we got a relatively high R-squared in our full model. From the result in Appendix
section - Results of Forward/Backward Stepwise Regression both forward and backward stepwise regression shows that
the model with all predictors has the best AIC value. It might be because we split the testing from our original data, which
might be very clean initially.</p>

<h3 id="44-residual-analysis---full-model">4.4 Residual Analysis - Full Model</h3>
<h4 id="441-linearity-assumption">4.4.1 Linearity Assumption</h4>
<p>The scatter plots below show the standardized residuals against each quantitative variable, <strong><em>Temperature</em></strong>,
<strong><em>Fuel_Price</em></strong>, <strong><em>Unemployment</em></strong>, and <strong><em>CPI</em></strong>. The data points are randomly scattered around 0. This shows that the Linearity
assumption holds for the full model.</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-4-1-1.png" alt="4-4-1-1" title="4-4-1-1" /></p>

<h4 id="442-constant-variance-assumption">4.4.2 Constant Variance Assumption</h4>
<p>Overall, the plots are scattered randomly around 0 in Figure 7. However, a few standardized residuals show larger
variance as the predicting variable increases. The constant variance assumption might not hold for the complete model.</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-4-2-1.png" alt="4-4-2-1" title="4-4-2-1" /></p>

<h3 id="443-normality-assumption">4.4.3 Normality Assumption</h3>
<p>The histogram of the standardized residuals for the full model is bell-shaped in the left chart of Figure 8. It shows
that the residuals should have an approximately symmetric and unimodal distribution. However, the curvature at the ends
of the Q-Q plot on the right of Figure 8 suggests a non-normality in the complete model.</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-4-3-1.png" alt="4-4-3-1" title="4-4-3-1" /></p>

<h3 id="45-outlier-checks---full-model">4.5 Outlier Checks - Full Model</h3>
<p>We conducted Cook’s Distance as a measure to exclude the outliers. The threshold we selected is 0.0078, retrieved
from 4 divided by the size of the sample dataset (=4/5148). In Figure 9, there are several data points that should be
removed from this criteria.</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-5-1.png" alt="4-5-1" title="4-5-1" /></p>

<h3 id="46-multicollinearity-diagnosis---full-model">4.6 Multicollinearity Diagnosis - Full Model</h3>
<p>For VIF (Variance Inflation Factor) analysis, we would like to take the degree of freedom into account for our
model since the number of categories for the qualitative variables is relatively large in the data set. The result shown in
Figure 10 indicates that the VIF value of <strong><em>CPI</em></strong> is significantly higher than other predicting variables. This suggests that
<strong><em>CPI</em></strong> might be more correlated with the other predictors. Thus, it should be taken out potentially.</p>

<p><img src="/assets/images/Walmart Sales Prediction/4-6-1.png" alt="4-6-1" title="4-6-1" /></p>

<h3 id="47-improvements---reduced-model">4.7 Improvements - Reduced Model</h3>
<p>Based on the residual analysis, outlier checking, and multicollinearity diagnosis presented in the previous
sections, we performed 3 improvements to acquire a better model.</p>

<ol>
  <li>Fitted the model with Square Root Transformation. We plan to eliminate the violation of assumptions by
fitting the model with square root transformation.</li>
  <li>Removed the outliers with Cook’s Distance larger than 0.0078. If there are a large number of outliers,
they probably point to a heavy tailed distribution rather than truly extreme values. To fix the assumption
violation issue, the outliers would be taken away.</li>
  <li>Removed the predictor <strong><em>CPI</em></strong>. We would like to improve R-Squared by excluding the predictor that is
correlated with the other predictors potentially.</li>
</ol>

<style>
p{
	text-align: justify;
}
</style>

:ET