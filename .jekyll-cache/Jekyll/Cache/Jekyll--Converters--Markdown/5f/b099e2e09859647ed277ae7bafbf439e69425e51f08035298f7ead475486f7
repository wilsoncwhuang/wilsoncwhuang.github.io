I"<h2 id="abstract">Abstract</h2>
<p>Loans are important to lenders and borrowers. It is important to identify the risky behaviors of clients and make educated decision. 
This paper is built on an end-to-end machine learning case study for predicting the defaulting risk associated with a borrower. 
The paper highlights on the exploratory data analysis of the datasets, feature engineering and machine learning modeling techniques and assess the results of these models.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>There has been a significant increase in the banking industry recently in which the unbanked population who are not privy to financial literacy are discriminated against when applying to loans. 
This means there’s an inequity in the field of loan granting that needs to be remedied. Our objective is to predict the likelihood of loan repayment to better distribute loans. 
We came across a dataset on Kaggle from HomeCredit whose mission is to broaden financial inclusion for unbanked populations.</p>

<h4 id="11-background">1.1 Background</h4>
<p>The dataset consisted of seven different sheets: current application, bureau balance, credit card balance, installments, cash balance and previous installments. 
The main objective is to identify the potential Defaulters based on the given data about the applicants. 
The probability of classification is essential because we want to be very sure when we classify someone as a Non-Defaulter, as the cost of making a mistake can be very high to the company. 
Our goal was to utilize important features from these sources to identify which would be key in the resultant machine learning model. Our methodology included cleansing the data, visualizing various elements to better understand it, feature selection, model fitting, and derive our final conclusions.</p>

<p>Since the dataset was created for a datathon, the testing data did not include the labeled response. Therefore, we decided to create a 70/30 split on the training set in order to test the accuracy of the model.</p>

<h2 id="2-methods">2. Methods</h2>

<h4 id="21-data-processing">2.1 Data Processing</h4>
<p>Data preparation is integral to deriving clear and accurate results from a machine learning model. There are specific properties we noted when taking a deep dive into the data. It was incredibly dense, spanning across ~450 columns in total and ~307,000 rows. 
There were inconsistencies in the data types within the columns as well meaning it was heterogeneous in nature. It was also time-varying and there were unbalances in some of the responses that required cleansing. 
The following flow was utilized to understand the scope of the data and determine which predictive model would be best.
<img src="/assets/images/Home Credit Default Risk/data preparation methodology.png" alt="data preparation methodology" title="data preparation methodology" /></p>

<h4 id="22-data-exploration">2.2 Data Exploration</h4>

<p>We first hypothesized which features may be most important in determining the likelihood of loan repayment. Working with a few, we derived the following:</p>

<p><img src="/assets/images/Home Credit Default Risk/data exploration 1.png" alt="data exploration 1" title="data exploration 1" /></p>

<p>We began by looking at a high-level scale of metrics to gauge what our overall responses might look like. 
As you can see, the percentage of people who don’t pay back their loan was very small, so it was important to identify the correct model and execute accurate feature selection to isolate the variables most impact likelihood of repayment. 
Here are some breakdowns of main factors we initially thought would strongly influence the model. Again, we see an even dispersion of the target across all groups of the factors observed. 
However, we believed that car ownership may be a strong indicator that an ability to pay off a car loan might signal an ability to pay off other types of loan.</p>

<p><img src="/assets/images/Home Credit Default Risk/data exploration 2.png" alt="data exploration 2" title="data exploration 2" /></p>

<p>In order to aggregate the data appropriately, we used the following schema to determine the primary and foreign keys necessary for database integration. 
This allowed us to access all the attributes associated with each customer simultaneously and manipulate the data accordingly.</p>

<p><img src="/assets/images/Home Credit Default Risk/data exploration 3.png" alt="data exploration 3" title="data exploration 3" /></p>

<p>The first order of business to find which attributes are positively and negatively correlated with our response. We found the following results.</p>

<p><img src="/assets/images/Home Credit Default Risk/data exploration 4.png" alt="data exploration 4" title="data exploration 4" /></p>

<p>As we can see, the influence of each factor seems incredibly small, but it’s crucial to note the number of factors under consideration during this analysis. 
Therefore, relatively, this insight is useful. We’ve included part of the myriad of breakdown we’ve built, including the relationships between correlation across age groups, and the variable known as ‘EXT_SOURCE_3’. 
Upon investigation, we discovered that a large part of the data sourcing comes from confidential financial information that has been tokenized for our use. 
They’ve been exported into these columns accordingly. This information is insightful particularly because it helps us better predict which attributes, we may observe as a result of our feature selection.</p>

<p><img src="/assets/images/Home Credit Default Risk/data exploration 5.png" alt="data exploration 5" title="data exploration 5" /></p>

<p>We then wanted to explore whether there were significant correlations between the attributes themselves. 
We looked through our table with the highest correlations to the response, and then built out heat maps based on their respective values. 
Since we previously analyzed impact on the target value, we wanted to isolate variables’ relationships to one another. 
This is particularly insightful to see if there was an aliasing of any kind of variable to one another. 
If you look at the figure associated with credit card balance, we find that there are a few extraneous highly correlated variables, but their exclusion in our final model did not impact our results, therefore we moved forward with the appropriate feature selection procedures.</p>

<p><img src="/assets/images/Home Credit Default Risk/data exploration 6.png" alt="data exploration 6" title="data exploration 6" /></p>

<h4 id="22-data-cleansing-and-feature-selection">2.2 Data Cleansing and Feature Selection</h4>
<p>We came across a significant number of columns that had missing values at a proportion that would be misrepresentative to keep in our model. 
For the columns that had over 60% missing data, we dropped the columns entirely. For those with less than 60%, but still a significant amount of missing data, we used the median of the columns in place of the null values. 
Here is an example of the list of columns that yielded these results.</p>

<style>
p{
	text-align: justify;
}
</style>

:ET